{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1b: Predicting survival of Titanic passengers with decision trees\n",
    "**DUE September 17th 2018**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The code for this project consists of several Python files, some of\n",
    "which you will need to read and understand in order to complete the\n",
    "assignment, and some of which you can ignore.\n",
    "\n",
    "### Files You'll Edit\n",
    "\n",
    "``assignment_1b.ipynb``: Will be your edited copy of this notebook pertaining to part 1a of the assignment\n",
    "\n",
    "``features.py``: Simple feature engineering function\n",
    "\n",
    "\n",
    "\n",
    "### Files you might want to look at\n",
    "  \n",
    "``binary.py``: Our generic interface for binary classifiers (actually\n",
    "works for regression and other types of classification, too).\n",
    "\n",
    "``datasets.py``: Where a handful of test data sets are stored.\n",
    "\n",
    "``util.py``: A handful of useful utility functions: these will\n",
    "undoubtedly be helpful to you, so take a look!\n",
    "\n",
    "``runClassifier.py``: A few wrappers for doing useful things with\n",
    "classifiers, like training them, generating learning curves, etc.\n",
    "\n",
    "``mlGraphics.py``: A few useful plotting commands\n",
    "\n",
    "``data/*``: all of the datasets we'll use.\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "You will hand in all of the python files listed above under \"Files\n",
    "you'll edit\". You will also have to answer the written questions in this\n",
    "notebook denoted **Q#:** in the corresponding cells denoted with **A#:**.\n",
    "\n",
    "#### Autograding\n",
    "\n",
    "Your code will be autograded for technical correctness. Please **do\n",
    "not** change the names of any provided functions or classes within the\n",
    "code, or you will wreak havoc on the autograder. However, the\n",
    "correctness of your implementation -- not the autograder's output --\n",
    "will be the final judge of your score.  If necessary, we will review\n",
    "and grade assignments individually to ensure that you receive due\n",
    "credit for your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `data/` you will find the following files:\n",
    "    `titanic_train.csv`\n",
    "        \n",
    "    `titanic_test.csv`\n",
    "    \n",
    "Let's take a look at the CSV file using the [Pandas] package and import other packages we think we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dt\n",
    "import features\n",
    "import runClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas lets us take read CSVs easily and allows us to manipulate the data with ease. So lets take a look at the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/titanic_train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each passenger is identified with a unique PassengerId and is labeled with whether or not she survived the Titanic accident. We can also see that we have some simple information about each of them. In each column, 1 signifies True and 0 False. Since the decision tree we have implemented is quite simple and knows to split on only binary features (either 1 or 0), we have preprocessed the data and have already binaraized some features for you. They are as follows:\n",
    "- `HighClassTicket`: Signifies whether or not the passenger bought a ticket with some extra perks\n",
    "- `IsOld`: Signifies whether or not the passenger is older than 22\n",
    "- `hasLargeFamily`: Signifies whether the passenger had more than 4 other family members on board\n",
    "- `isSingle`: Signifies whether the passenger had no other family members on board\n",
    "- `hadNiceCabin`: Signifies whether the passenger purchased an upgraded cabin\n",
    "- `isAristocrat`: Signifies whether the passenger had an aristocratic title in his/her name (E.g. Sir, Lord, Dutchess etc.)\n",
    "\n",
    "However, you have to do some **feature engineering** and 'binarize' the remainding columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the simple decision tree that we implemented does not know how to find partitions in features that are strings or features that are continuous. We will have to do some **feature engineering** to solve this. Binarizing the `Sex` feature is simple. However you will have to figure out a reasonable threshold for binarizing the `Fare` feature.\n",
    "\n",
    "Do some data analysis below to find a reasonbable. **Plot a chart**  and **explain** why you chose the threshold you chose.\n",
    "(Hint: Use histograms, analyze the survival rates and make a reasobable guess. Or find the threshold that minimizes impurity!)\n",
    "\n",
    "\n",
    "Also, **complete** the `binarize_features` function in `binarize.py`, this function should return a Pandas dataframe with the same number of columns, and binarize the `Fare` and `Sex` columns to `int`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO; Insert your analysis here. Add more cells if you need to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** Why did you threshhold/binarize the `Fare` featuer at that value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1:** (TODO: Your answer here...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = features.binarize_features(train_df, 25)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is a test csv, we won't always have access to labels in our test data. Instead, we hold out a portion (20%) of our training data to help us measure how generalizable the trained model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_df.iloc[:,2:].values, train_df.iloc[:,1].values\n",
    "X_holdout, y_holdout, X_train, y_train = train_test_split(X, y, test_size=0.2, random_state=422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `hyperparameterCurve` in `runClassifier.py`, **plot** a corresponding chart of tree depth vs accuracy and **choose** the best tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert code and analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** According to your analysis, what is the best tree depth? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2:** (TODO: Your answer here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now let's retrain on all the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt.DT({'maxDepth': #insert value here })\n",
    "dt.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Q3:** Why would we want to retrain a decision tree on all the data (`X` and `y`) and not just `X_train` and `y_train`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3:** (TODO: Your answer here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our decision tree the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/titanic_test.csv')\n",
    "test_df = features.binarize_features(test_df, #insert value here )\n",
    "X_test, y_test = test_df.iloc[:,2:].values, test_df.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = dt.predictAll(X_test)\n",
    "acc = np.mean(y_test == y_predicted)\n",
    "print(\"Test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
