{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1c: Information Gain for Decision Trees\n",
    "**DUE September 17th 2018**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The code for this project consists of several Python files, some of\n",
    "which you will need to read and understand in order to complete the\n",
    "assignment, and some of which you can ignore.\n",
    "\n",
    "### Files You'll Edit\n",
    "\n",
    "``assignment_1c.ipynb``: Will be your edited copy of this notebook pertaining to part 1c of the assignment.\n",
    "\n",
    "### Files you might want to look at\n",
    "  \n",
    "``binary.py``: Our generic interface for binary classifiers (actually\n",
    "works for regression and other types of classification, too).\n",
    "\n",
    "``datasets.py``: Where a handful of test data sets are stored.\n",
    "\n",
    "``util.py``: A handful of useful utility functions: these will\n",
    "undoubtedly be helpful to you, so take a look!\n",
    "\n",
    "``runClassifier.py``: A few wrappers for doing useful things with\n",
    "classifiers, like training them, generating learning curves, etc.\n",
    "\n",
    "``mlGraphics.py``: A few useful plotting commands\n",
    "\n",
    "``data/*``: all of the datasets we'll use.\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "You will hand in all of the python files listed above under \"Files\n",
    "you'll edit\". You will also have to answer the written questions in this\n",
    "notebook denoted **Q#:** in the corresponding cells denoted with **A#:**.\n",
    "\n",
    "#### Autograding\n",
    "\n",
    "Your code will be autograded for technical correctness. Please **do\n",
    "not** change the names of any provided functions or classes within the\n",
    "code, or you will wreak havoc on the autograder. However, the\n",
    "correctness of your implementation -- not the autograder's output --\n",
    "will be the final judge of your score.  If necessary, we will review\n",
    "and grade assignments individually to ensure that you receive due\n",
    "credit for your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write code to support the use of the information gain splitting criterion for decision tree learning. **You should now complete all lines marked ``TODO`` in ``dt.py``, so that our code handles both splitting criteria (misclassification rate and information gain).** Once you've done that, we can test our code for the new splitting criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dumbClassifiers, datasets\n",
    "import numpy as np\n",
    "import runClassifier\n",
    "import dtSol as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dt.DT({'maxDepth': 1, 'criterion': 'ig'})\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO: Implement dt.train(...)                                                #\n",
    "################################################################################\n",
    "h.train(datasets.TennisData.X, datasets.TennisData.Y)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for a simple depth-one decision tree (aka a decision stump). Notice how we print the information gain corresponding to each branch in the tree.\n",
    "\n",
    "If we let it get deeper, we get things like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dt.DT({'maxDepth': 2, 'criterion': 'ig'})\n",
    "h.train(datasets.TennisData.X, datasets.TennisData.Y)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dt.DT({'maxDepth': 5, 'criterion': 'ig'})\n",
    "h.train(datasets.TennisData.X, datasets.TennisData.Y)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar on the sentiment data (this will take a bit longer---it takes about 10 seconds on my laptop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dt.DT({'maxDepth': 2, 'criterion': 'ig'})\n",
    "h.train(datasets.SentimentData.X, datasets.SentimentData.Y)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look up the words (your results here might be different due to hashing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(626, datasets.SentimentData.words[626])\n",
    "print(683, datasets.SentimentData.words[683])\n",
    "print(1627, datasets.SentimentData.words[1627])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we can rewrite the tree (by hand) as:\n",
    "\n",
    "```python\n",
    "Branch 'bad'\n",
    "  Branch 'worst'\n",
    "    Leaf 1.0\n",
    "    Leaf -1.0\n",
    "  Branch 'stupid'\n",
    "    Leaf -1.0\n",
    "    Leaf -1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will test prediction (this takes about a minute for me):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO: Implement dt.predict(...)                                              #\n",
    "################################################################################\n",
    "runClassifier.trainTestSet(dt.DT({'maxDepth': 1, 'criterion': 'ig'}), datasets.SentimentData)\n",
    "runClassifier.trainTestSet(dt.DT({'maxDepth': 3, 'criterion': 'ig'}), datasets.SentimentData)\n",
    "runClassifier.trainTestSet(dt.DT({'maxDepth': 5, 'criterion': 'ig'}), datasets.SentimentData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use more ``runClassifier`` functions to generate learning\n",
    "curves and hyperparameter curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curveIG = runClassifier.learningCurveSet(dt.DT({'maxDepth': 9, 'criterion': 'ig'}), datasets.SentimentData)\n",
    "runClassifier.plotCurve('DT on Sentiment Data', curveIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots training and test accuracy as a function of the number of\n",
    "data points (x-axis) used for training and y-axis is accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare information gain with misclassification rate. First we'll generate the learning curve for misclassification rate again. Then we'll plot the curves on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curveMR = runClassifier.learningCurveSet(dt.DT({'maxDepth': 9, 'criterion': 'mr'}), datasets.SentimentData)\n",
    "runClassifier.plotCurvePair('DT on Sentiment Data', curveIG, 'IG', curveMR, 'MR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** Briefly compare the two **training** curves. Does either splitting criterion perform better than the other for small dataset size (say, N<200)? Why or why not? How about as N increases to 1200? Use your understanding of both criteria to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1:** (TODO: Enter answer here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Briefly compare the two **training** curves. Does either splitting criterion lead to better generalization than the other for small dataset size (say, N<200)? Why or why not? How about as N increases to 1200? Use your understanding of both criteria to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2:** (TODO: Enter answer here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate similar curves by changing the maximum depth hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curveIG = runClassifier.hyperparamCurveSet(dt.DT({'criterion': 'ig'}), 'maxDepth', [1,2,4,6,8,12,16], datasets.SentimentData)\n",
    "curveMR = runClassifier.hyperparamCurveSet(dt.DT({'criterion': 'mr'}), 'maxDepth', [1,2,4,6,8,12,16], datasets.SentimentData)\n",
    "runClassifier.plotCurvePair('DT on Sentiment Data (hyperparameter)', curveIG, 'IG', curveMR, 'MR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the x-axis is the value of the maximum depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Briefly compare the two **training** curves. Does either splitting criterion perform better than the other for shallow depth (say, ``maxDepth``<10)? Why or why not? How about as ``maxDepth`` increases to 16? Use your understanding of both criteria to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A3:** (TODO: Enter answer here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** Briefly compare the two **test** curves. Does either splitting criterion lead to better generalization than the other for shallow depth (say, ``maxDepth``<10)? Why or why not? How about as ``maxDepth`` increases to 16? Use your understanding of both criteria to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A4:** (TODO: Enter answer here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will display a tree trained using information gain. Beside each branch, we print out the information gain corresponding to the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dt.DT({'maxDepth': 5, 'criterion': 'ig'})\n",
    "h.train(datasets.SentimentData.X, datasets.SentimentData.Y)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some of the features, so we can see which words this tree uses to make decisions. I've looked up word indexed at ``626``, but you can go ahead and edit as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(626, datasets.SentimentData.words[626])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Look up some words used in the tree. Find a few representative words that seem _helpful_ (contributing to higher accuracy) in this classification task, and a few representative words that seem _unhelpful_. Do you notice a correlation between information gain and the quality of the word in this task? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5:** (TODO: Enter answer here...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Should we expect a significant change in test accuracy if we prune subtrees rooted at nodes corresponding to low information gain? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A5:** (TODO: Enter answer here...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
